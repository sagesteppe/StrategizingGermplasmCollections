#' Create environmental and spatial clusters for targeting collection areas
#' 
#' This function utilizes the output from an elastic net GLM model to create a weights matrix of
#' features relevant to a species distribution to identify clusters throughout it's range while
#' incorporating the PCNM/MEM data and coordinates to implement some spatial contiguity. 
#' 
#' @param pred_rescale a rasterstack of predictor layers which have been rescaled to represent the beta coefficients from the elastic net (glmnet::glmnet) modelling process. See ?RescaleRasters for an implementation of this functionality. 
#' @param f_rasts The rasters output from the SDM workflow. 
#' @param path a root path where each output data will be saved, use the same as in `WriteSDMresults`.
#' @param taxon Character. the name of the taxonomic entity for which the models were created. The final raster of clusters, the results from both KNN classifier trainings, and details of the clustering procedure (if fixedClusters=TRUE).
#' @param n Numeric. the number of clusters desired. 
#' @param fixedClusters Boolean. Defaults to TRUE, which will create n clusters. If False then use NbClust::NbClust to determine the number of clusters.
#' @param n_pts Numeric. the number of points to use for generating the clusters, these will be randomly sampled within the mask area `mask`. Defaults to 500. 
#' @param planar_proj Numeric, or character vector. An EPSG code, or a proj4 string, for a planar coordinate projection, in meters, for use with the function. For species with very narrow ranges a UTM zone may be best (e.g. 32611 for WGS84 zone 11 north, or 29611 for NAD83 zone 11 north). Otherwise a continental scale projection like 5070 See https://projectionwizard.org/ for more information on CRS. The value is simply passed to sf::st_transform if you need to experiment. 
#' @param coord_wt Numeric. The amount to weigh coordinates by for the distance matrix relative to the most important variable identified by elastic net regression. Defaults to 2.5, setting at 1 would make this value equivalent to environmental and PCNM variables. This metric increases the spatial contiguity of the clusters identified. 
#' @param buffer_d Numeric. When using two-stage sampling to increase the sample size (number of points) for uncommon clusters, the distance to buffer the individual pts which are located in these cells for further possible sampling. Defaults to 3 which allows for each an area encompassing around 45-50 raster cells nearby to possibly be sampled. This is a reasonable default for coarsely gridded data (the most sensible grain type for this packages use cases), moderate resolution data (e.g. 250m-1km) may require higher values to find meaningful differences between the variables at these further locations. 
#' @param prop_split Numeric. The proportion of records to be used for training the KNN classifier. Defaults to 
#' 0.8 to use 80% of records for training and 20% for the independent test sample. 
#' @param ... Further arguments passed to NbClust::NbClust for optimizing cluster numbers. Defaults to using method = 'complete', which will compare the results from 20 methods and select the cluster number most commonly generated by all of these algorithms. We have min.nc set as a default of 5, to overcome having too few clusters to use (but this easily overwritten by supplying the argument minc.nc=2, to set it lower), and max.nc = 20 for congruence with many seed collection endeavors (overwritten by max.nc = 10 for example).
#' @export
EnvironmentalBasedSample <- function(pred_rescale, f_rasts, taxon, path, n, fixedClusters, n_pts, planar_proj, coord_wt, buffer_d, prop_split){
  
  if(missing(fixedClusters)){fixedClusters <- TRUE}
  if(missing(prop_split)){prop_split <- 0.8}
  if(missing(n_pts)){n_pts <- 500}
  if(missing(buffer_d)){buffer_d <- 3}
  if(missing(coord_wt)){coord_wt <- 2.5}
  
  ####### add coordinates to make them an explicit feature in the clustering #####
  ranges <- terra::global(pred_rescale, fun = 'range', na.rm = TRUE)
  targetRangeCoords <- max(abs(ranges$min - ranges$max)) * coord_wt
  
  pred_rescale$x <- terra::init(pred_rescale, fun = 'x') 
  pred_rescale$y <- terra::init(pred_rescale, fun = 'y')
  
  pred_rescale$x <- scale(pred_rescale$x)
  pred_rescale$y <- scale(pred_rescale$y)
  
  coordRanges <- rbind(
    terra::global(pred_rescale$y, fun = 'range'),
    terra::global(pred_rescale$x, fun = 'range')
  )
  
  coordRanges$range = (coordRanges$min - coordRanges$max) 
  coordRanges$multiplier = targetRangeCoords/ coordRanges$range 
  
  pred_rescale$x <- pred_rescale$x * coordRanges[rownames(coordRanges)=='x','multiplier']
  pred_rescale$y <- pred_rescale$y * coordRanges[rownames(coordRanges)=='y','multiplier']
  pred_rescale <- terra::mask(pred_rescale, pred_rescale[[1]]) # mask to areas of interest
  
  rm(targetRangeCoords, coordRanges)
  
  ######## now drop any predictors which were shrunk out of the model ###############
  # note we need the TRUE on the end for the coord layers we just added. 
  pred_rescale <- pred_rescale [[ c(ranges$min != ranges$max, rep(TRUE, times = 2)) ]]
  
  ## extract preds to points for creating matrices. ##
  pts <- terra::spatSample(
    f_rasts[['Supplemented']], 
    as.points = TRUE,
    method = 'random', size = 500, na.rm = TRUE)
  
  weighted_mat <- terra::extract(
    pred_rescale, pts, bind = TRUE) |> 
    as.data.frame() |>
    dplyr::select(-Supplemented) 
  
  if(fixedClusters==TRUE){ # run the clustering processes. 
    
    w_dist <- dist(weighted_mat, method = 'euclidean')
    clusters <- hclust(w_dist, method = 'ward.D2')
    clusterCut <- cutree(clusters, n)
    
  } else {

    NoClusters <- NbClust::NbClust(
      data = weighted_mat, diss = w_dist, 
      distance = NULL, min.nc = min.nc, max.nc = max.nc, 
      method = 'complete'
    )
    clusterCut <- NoClusters$Best.partition
  }
  
  # Prepare data for training the initial KNN classifier #
  weighted_mat$ID <- factor(clusterCut)
  weighted_mat <- weighted_mat[complete.cases(weighted_mat),]
  
  firstKNN <- trainKNN(weighted_mat, split_prop = prop_split)
  fit.knn <- firstKNN$fit.knn
  
  ########             MAKE DATA SET MORE BALANCED.            #######
  
  # let's sample our original raster again, but this time, let's search in the
  # geographic spaces where we obtained these groups members from - then we'll
  # run the clustering process again and hope we get vaguely similar groups with 
  # similar sample sizes. 
  
  # any point with fewer than the median number of observations will be feed back in
  cc <- table(clusterCut)
  more_samples <- as.numeric(which(cc < median(cc))) # these need more sample
  
  # determine how large each cell is in m, we can use this as a basis for the buffering process. 
  r_projected <- f_rasts[['Supplemented']][[1]] |>
    terra::project(planar_proj)
  d <- terra::xres(r_projected)
  
  # need to use the untransformed points to get the proper sizes. 
  need_more_samples <- pts[ weighted_mat$ID %in% more_samples, c('x', 'y')] |>
    sf::st_as_sf(coords = c(x='x', y='y'), crs = terra::crs(f_rasts[['Supplemented']]))  |>
    sf::st_transform(crs = terra::crs(r_projected)) |>
    sf::st_buffer(d * buffer_d) |>  
    dplyr::summarize(geometry = sf::st_union(geometry)) |>
    sf::st_make_valid() 
  
  concentrated_pts <- sf::st_sample(need_more_samples, size = n_pts/5, type = 'regular') |>
    sf::st_as_sf() |>
    sf::st_transform( terra::crs(f_rasts[['Supplemented']]) )
  
  concentrated_pts <- terra::extract(
    pred_rescale, concentrated_pts,  bind = TRUE, 
  ) |>
    as.data.frame()
  
  concentrated_pts <- concentrated_pts[complete.cases(concentrated_pts),]
  concentrated_pts <- unique(concentrated_pts)
  concentrated_pts$ID <- 1:nrow(concentrated_pts)
  
  # now we see if any points from the first data set are repeated in the second
  duplicated <- dplyr::inner_join( # obviously we remove them... 
    concentrated_pts[,c('ID', 'x', 'y')], weighted_mat[,c('x', 'y')])
  concentrated_pts <- concentrated_pts[ 
    ! concentrated_pts$ID %in% duplicated$ID, 
    1:ncol(concentrated_pts)-1]
  
  ######## NOW APPLY A K NEAREST NEIGHBORS ALGORITHM TO THESE POINTS AND SEE
  # WHICH GROUPS THEY BELONG ##
  concentrated_pts$ID <- as.numeric(predict(fit.knn, newdata = concentrated_pts))
  concentrated_pts <- concentrated_pts [ concentrated_pts$ID %in% more_samples, ] 
  
  ## RUN THE FINAL CLASSIFICATION PASS. 
  ## prepare data to train a second KNN with all added data. 
  weighted_mat <- rbind(concentrated_pts, weighted_mat)
  weighted_mat$ID <- factor(weighted_mat$ID)
  
  rm(concentrated_pts, d, need_more_samples, r_projected)
  
  finalKNN <- trainKNN(weighted_mat, split_prop = prop_split)
  final.fit <- finalKNN$fit.knn
  confusionMatrix <- final.fit$confusionMatrix
  
  
  spatialClusters <- terra::predict(pred_rescale, model = fit.knn, na.rm = TRUE)
  spatialClusters <- terra::mask(spatialClusters, f_rasts[['Supplemented']])
  
  #Clean up products for distribution
  
  ClusterVectors <- terra::as.polygons(spatialClusters) |>
    sf::st_as_sf() |>
    sf::st_make_valid() |>
    dplyr::mutate(class = as.numeric(class)) |>
    dplyr::arrange(class)
  
  dir.create(file.path(path, 'ClusterRasters'), showWarnings = FALSE)
  dir.create(file.path(path, 'ClusterVectors'), showWarnings = FALSE)
  
  terra::writeRaster(spatialClusters, overwrite = TRUE,
                     file.path(path, 'ClusterRasters', paste0(taxon, '.tif')))
  
  dir.create(file.path(path, 'TrainingKNN'), showWarnings = FALSE)
  saveRDS( # save the final fitted model.
    final.fit, 
    file = file.path(path, 'TrainingKNN', paste0(taxon, '-finalKNN.rds'))
  )
  saveRDS( # save the final fitted model.
    confusionMatrix, 
    file = file.path(path, 'TrainingKNN', paste0(taxon, '-finalCM.rds'))
  )
  sf::st_write(
    ClusterVectors, 
    file.path(path, 'ClusterVectors', paste0(taxon, '.shp')), 
    append = FALSE, quiet = TRUE)
  
}
